# üöÄ GUIA R√ÅPIDO - Ollama + Mistral

## ‚ùå PROBLEMAS IDENTIFICADOS

### 1. Ollama n√£o estava rodando
```powershell
# Solu√ß√£o: Iniciar servi√ßo Ollama
ollama serve
```

### 2. Nenhum modelo instalado
```bash
# Voc√™ executou: ollama list
# Resultado: NAME    ID    SIZE    MODIFIED (vazio!)
```

### 3. Mistral ‚â† "Magister"
- ‚úÖ **Mistral** existe e est√° sendo baixado agora
- ‚ùå **Magister** N√ÉO √© um modelo oficial do Ollama
- üí° Voc√™ provavelmente quis dizer **Mistral** ou **Mistral-OpenOrca**

---

## ‚úÖ SOLU√á√ÉO IMPLEMENTADA

### Arquivos criados:
1. ‚úÖ `server.py` - Proxy FastAPI completo
2. ‚úÖ `install_models.ps1` - Script PowerShell para Windows
3. ‚úÖ `install_models.sh` - Script atualizado para Linux/Mac

### Modelo sendo instalado:
```
üì¶ Mistral 7B (4.1GB) - 27% conclu√≠do
Estimativa: ~11 minutos restantes
```

---

## üéØ COMO USAR (ap√≥s download)

### 1. Verificar modelos instalados
```powershell
ollama list
```

### 2. Testar Mistral diretamente
```powershell
ollama run mistral "Explique machine learning em 50 palavras"
```

### 3. Iniciar o Avila AI Proxy
```powershell
cd d:\Vizzio\crates\avila-ai-proxy
pip install -r requirements.txt
python server.py
```

### 4. Testar via API
```powershell
# Health check
curl http://localhost:8000/health

# Listar modelos
curl http://localhost:8000/v1/models

# Chat com Mistral
curl -X POST http://localhost:8000/v1/chat/completions `
  -H "Content-Type: application/json" `
  -d '{
    "model": "mistral",
    "messages": [
      {"role": "user", "content": "Ol√°! Voc√™ √© o Mistral?"}
    ]
  }'
```

---

## üìä MODELOS DISPON√çVEIS NO OLLAMA

### Modelos base (recomendados):
| Nome | Tamanho | Descri√ß√£o |
|------|---------|-----------|
| `mistral` | 4.1GB | ‚≠ê Melhor custo-benef√≠cio |
| `llama3.2` | 2.0GB | Mais r√°pido, menor qualidade |
| `dolphin-mistral` | 4.1GB | Sem censura (pesquisa) |
| `codellama` | 3.8GB | Especializado em c√≥digo |

### Instalar outros modelos:
```powershell
ollama pull llama3.2
ollama pull codellama
ollama pull dolphin-mistral
```

---

## üîß TROUBLESHOOTING

### Erro: "N√£o √© poss√≠vel estabelecer liga√ß√£o"
```powershell
# Ollama n√£o est√° rodando
# Solu√ß√£o:
ollama serve
```

### Erro: "model not found"
```powershell
# Modelo n√£o instalado
# Solu√ß√£o:
ollama pull mistral
```

### Servidor Python n√£o inicia
```powershell
# Depend√™ncias faltando
# Solu√ß√£o:
pip install fastapi uvicorn httpx pydantic python-multipart websockets
```

---

## üí° DIFEREN√áA MISTRAL vs "MAGISTER"

- ‚úÖ **Mistral** = Modelo real do Ollama (4.1GB)
- ‚ùå **Magister** = N√ÉO EXISTE no Ollama Hub oficial
- ü§î Poss√≠veis confus√µes:
  - **Mistral-OpenOrca** (variante otimizada)
  - **Mistral-Nemo** (vers√£o maior)
  - **Mixtral** (modelo de 47GB com 8 experts)

---

## üöÄ PR√ìXIMOS PASSOS

Quando o download do Mistral terminar (ainda faltam ~11 min):

```powershell
# 1. Verificar instala√ß√£o
ollama list

# 2. Testar localmente
ollama run mistral "Teste de funcionamento"

# 3. Iniciar proxy
cd d:\Vizzio\crates\avila-ai-proxy
python server.py

# 4. Usar via API (OpenAI-compatible)
curl http://localhost:8000/v1/chat/completions -X POST ...
```

---

## üìö REFER√äNCIAS

- [Ollama Models Library](https://ollama.ai/library)
- [Mistral AI Docs](https://docs.mistral.ai/)
- [Avila AI Proxy API](http://localhost:8000/docs) (ap√≥s iniciar)

---

**Status atual:** ‚è≥ Aguardando download do Mistral (27% conclu√≠do)
**ETA:** ~11 minutos
**Pr√≥ximo:** Instalar depend√™ncias Python e iniciar servidor
